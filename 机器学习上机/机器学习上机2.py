import numpy as np
import pandas as pd
import streamlit as st
from math import log
from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score,auc,roc_curve
from sklearn.model_selection import train_test_split
import streamlit as st
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold

class Bayes_classfier():
    def __init__(self,lamda=1):
        self.X_train=None
        self.y_train=None
        self.N=0
        self.lamda=lamda


        self.pos_priors=None
        self.nege_priors=None
        self.pos_condition_likehood=None
        self.nege_condition_likehood=None
        self.one_minus_pos_condition_likehood=None
        self.one_minus_nege_condtion_likehood=None

    def Check_Format(self):

        x_type_str = type(self.X_train)
        y_type_str=type(self.y_train)
        # æ£€æŸ¥è¾“å…¥çš„è®­ç»ƒæ•°æ®æ ¼å¼
        if not x_type_str is np.ndarray :
            print('è¾“å…¥çš„è®­ç»ƒæ•°æ®è¦æ±‚numpyæ•°ç»„ï¼Œæ‚¨è¾“å…¥çš„æ˜¯ :{}'.format(type(self.X_train)))
            return False

        #æ£€æŸ¥è¾“å…¥çš„æ ‡ç­¾æ ¼å¼
        is_list=y_type_str is list
        is_np=y_type_str is np.ndarray
        if is_list==False and is_np==False:
            print("è¾“å…¥çš„æ ‡ç­¾è¦æ±‚numpyæ•°ç»„æˆ–è€…åˆ—è¡¨")
            return False

        if y_type_str is list:
            self.y_train=np.array(self.y_train)

        self.y_train=self.y_train.reshape(-1,1)

        #è®°å½•æ ·æœ¬æ•°
        sample_nums=self.y_train.shape[0]

        if self.X_train.shape[0]!=sample_nums and self.X_train.shape[0]!=sample_nums:
            print("è¾“å…¥çš„æ•°æ®é‡ä¸æ ‡ç­¾é‡ä¸åŒ¹é…")
            return  False
        return True

    def fit(self,X_train,y_train):
        self.X_train=X_train
        self.y_train=y_train

        if self.Check_Format()==False:
            print("è®­ç»ƒå¤±è´¥,æ£€æŸ¥è¾“å…¥æ ¼å¼")
            return

        #ç»Ÿè®¡æ ·æœ¬æ•°
        self.N=self.y_train.shape[0]

        #ä¼¯åŠªåˆ©æ¨¡å‹
        self.X_train[self.X_train!=0]=1

        #è¦è®¡ç®—æ¯ä¸ªç±»åˆ«ä¸­ï¼Œæ¯ä¸ªè¯å‡ºç°çš„æ€»æ•°
        #é¦–å…ˆæŠŠæ­£è´Ÿç±»åˆ†åˆ«å–å‡ºæ¥
        postive_class_idx=np.where(self.y_train==1)[0]
        negetive_class_idx=np.where(self.y_train==0)[0]

        postive_class=self.X_train[postive_class_idx]
        negetive_class=self.X_train[negetive_class_idx]
        pos_nums=len(postive_class)
        nege_nums=len(negetive_class)

        #è®¡ç®—æ¯ä¸ªè¯å€¼ä¸º1åœ¨æ¯ä¸ªç±»åˆ«ä¸‹å‡ºç°çš„æ¬¡æ•°
        pword_static=np.sum(postive_class,axis=0)
        nword_static=np.sum(negetive_class,axis=0)

        #è®¡ç®—æ¯ä¸€ä¸ªç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡
        self.pos_priors=pos_nums/self.N
        self.nege_priors=nege_nums/self.N

        #è®¡ç®—æ¡ä»¶æ¦‚ç‡p(x=xj|y=ck),é‡‡ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
        #p(x=1|y=ck),ä¹Ÿå°±æ˜¯æŸä¸ªç‰¹å¾å–1æ—¶å€™çš„æ¡ä»¶æ¦‚ç‡
        self.pos_condition_likehood=(pword_static+self.lamda)/(pos_nums+self.lamda*2)
        self.nege_condition_likehood=(nword_static+self.lamda)/(nege_nums+self.lamda *2)

        #p(x=0|y=ck)ï¼Œä¹Ÿå°±æ˜¯æŸä¸ªç‰¹å¾å–0æ—¶å€™çš„æ¡ä»¶æ¦‚ç‡
        self.one_minus_pos_condition_likehood=(self.N-pword_static)+self.lamda/(pos_nums+self.lamda*2)
        self.one_minus_nege_condtion_likehood=(self.N-nword_static)+self.lamda/(nege_nums+self.lamda *2)


    def predict(self,x_test:np.ndarray):
        #ç‰¹å¾äºŒå€¼åŒ–
        x_test[x_test>0]=1
        y_pred = np.zeros(x_test.shape[0], dtype=int)

        # è®¡ç®—å±äºæŸä¸ªç±»å¾—æ¦‚ç‡ï¼Œç”¨å¯¹æ•°è®¡ç®—æŠŠä¹˜æ³•å˜æˆåŠ æ³•ã€‚å¯¹äºæ¯ä¸ªç‰¹å¾p(xj=aj|y)=(1-x)*p(x=1)+x*p(x=0)å®ç°ç‰¹å¾çš„ä¸åŒå–å€¼è¿ä¹˜
        pos_likelihoods = np.sum(np.log(self.pos_condition_likehood) * x_test, axis=1) + np.sum(np.log(self.one_minus_pos_condition_likehood) * (1 - x_test), axis=1) + np.log(self.pos_priors)
        neg_likelihoods = np.sum(np.log(self.nege_condition_likehood) * x_test, axis=1) + np.sum(np.log(self.one_minus_nege_condtion_likehood) * (1 - x_test), axis=1) +np.log(self.nege_priors)

        # ç¡®å®šç±»åˆ«
        y_pred[pos_likelihoods > neg_likelihoods] = 1
        return y_pred

class Node:
    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):
        self.feature_idx = feature_idx # index of feature used for splitting
        self.threshold = threshold # threshold used for splitting
        self.left = left # left subtree
        self.right = right # right subtree
        self.value = value #

class DecisionTree:
    def __init__(self, max_depth=None, min_samples_split=2):
        self.max_depth = max_depth # æœ€å¤§æ·±åº¦
        self.min_samples_split = min_samples_split # æœ€å°åˆ‡åˆ†
        self.tree = None # æ ‘æ ¹

    def fit(self, X, y):
        #yå˜æˆä¸€ç»´æ•°ç»„
        y = y.ravel()
        self.tree = self._build_tree(X, y)

    def predict(self, X):
        return np.array([self._predict_tree(x, self.tree) for x in X])

    def _build_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))

        # åœæ­¢æ¡ä»¶
        if (self.max_depth is not None and depth == self.max_depth) or \
           (n_samples < self.min_samples_split) or \
           (n_classes == 1):
            value = np.argmax(np.bincount(y))
            return Node(value=value)

        # é€‰æ‹©æœ€ä¼˜åˆ‡åˆ†èŠ‚ç‚¹
        best_feature_idx, best_threshold = self._find_best_split(X, y, n_samples, n_features)

        # åˆ›å»ºèŠ‚ç‚¹
        left_idxs = np.where(X[:, best_feature_idx] <= best_threshold)[0]
        right_idxs = np.where(X[:, best_feature_idx] > best_threshold)[0]
        left = self._build_tree(X[left_idxs], y[left_idxs], depth+1)
        right = self._build_tree(X[right_idxs], y[right_idxs], depth+1)
        return Node(best_feature_idx, best_threshold, left, right)

    def _find_best_split(self, X, y, n_samples, n_features):
        best_info_gain = -1
        best_feature_idx = None
        best_threshold = None

        for feature_idx in range(n_features):
            feature_values = X[:, feature_idx]
            thresholds = np.unique(feature_values)
            for threshold in thresholds:
                info_gain = self._info_gain(y, feature_values, threshold, n_samples)
                if info_gain > best_info_gain:
                    best_info_gain = info_gain
                    best_feature_idx = feature_idx
                    best_threshold = threshold

        return best_feature_idx, best_threshold

    def _info_gain(self, y, feature_values, threshold, n_samples):
        parent_entropy = self._entropy(y, n_samples)
        left_idxs = np.where(feature_values <= threshold)[0]
        right_idxs = np.where(feature_values > threshold)[0]


        if len(left_idxs) == 0 or len(right_idxs) == 0:
            return 0

        # è®¡ç®—å­èŠ‚ç‚¹çš„ä¿¡æ¯ç†µ
        n_left = len(left_idxs)
        n_right = len(right_idxs)
        entropy_left = self._entropy(y[left_idxs], n_left)
        entropy_right = self._entropy(y[right_idxs], n_right)

        # è®¡ç®—Gï¼ˆD|a)
        child_entropy = (n_left / n_samples) * entropy_left + (n_right / n_samples) * entropy_right

        # è®¡ç®—åŸºå°¼æŒ‡æ•°
        info_gain = parent_entropy - child_entropy
        return info_gain

    def _entropy(self, y, n_samples):
        _, counts = np.unique(y, return_counts=True)
        probs = counts / n_samples
        entropy = -np.sum(probs * np.log2(probs))
        return entropy

    def _predict_tree(self, x, node):
        if node.value is not None:
            return node.value
        if x[node.feature_idx] <= node.threshold:
            return self._predict_tree(x, node.left)
        else:
            return self._predict_tree(x, node.right)

class KNN:
    def __init__(self, k=3):
        self.k = k

    def fit(self, X, y):
        y=y.ravel()
        self.X_train = X
        self.y_train = y

    def predict(self, X_test):
        y_pred = np.zeros(X_test.shape[0])

        # loop over all test examples
        for i, x_test in enumerate(X_test):
            # è®¡ç®—ä¸è®­ç»ƒé›†çš„è·ç¦»
            distances = np.sqrt(np.sum((self.X_train - x_test) ** 2, axis=1))

            #è·å–æœ€é‚»è¿‘
            knn_indices = np.argsort(distances)[:self.k]

            #è·å–æœ€è¿‘é‚»ç±»åˆ«
            knn_labels = self.y_train[knn_indices]

            # åˆ†ç±»
            y_pred[i] = np.bincount(knn_labels).argmax()

        return y_pred


st.set_page_config(page_title = 'æœºå™¨å­¦ä¹ è¯¾ç¨‹ä½œä¸š',page_icon = 'ğŸ•µï¸â€â™€ï¸',layout = 'wide',initial_sidebar_state='expanded')
st.title('æœºå™¨å­¦ä¹ è¯¾ç¨‹ä½œä¸š2')
st.sidebar.title("è¯¾ç¨‹ä¿¡æ¯")
st.sidebar.info("ç­çº§ï¼š196202")
st.sidebar.info("å§“åï¼šå§¬è¶Šåš")
st.sidebar.info("å­¦å·ï¼š20201000652")
uploaded_file = st.file_uploader("è¯·é€‰æ‹©è¦ä¸Šä¼ çš„CSVæ–‡ä»¶ï¼š", type="csv")


def Uploaded():
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)

        temp=st.text("æ­£åœ¨åŠ è½½æ•°æ®")
        temp=st.text("æ•°æ®åŠ è½½å®Œæˆ")


        st.write("æ‚¨çš„æ•°æ®å¦‚ä¸‹ï¼š")
        st.dataframe(df.describe())

        return df
    return None

def process(df,mode=0):

    Y = df[['target']]
    X = df.drop(columns=['target'])

    X = np.array(X)
    Y = np.array(Y)
    X_train, x_test, Y_train, y_test = train_test_split(X, Y, test_size=0.33)

    st.success("æ•°æ®å¤„ç†å®Œæˆ")

    return X_train, x_test, Y_train, y_test

def str_toInt(s):
    return int(float(s))

k = 5

# åˆå§‹åŒ–KæŠ˜äº¤å‰éªŒè¯
kf = KFold(n_splits=k, shuffle=True)


def K_fold_validation(X,y,model):
    st.write("ä¸‹é¢è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯")

    scores = []

    for train_idx, test_idx in kf.split(X):
        # Get the training and testing data for this fold
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        model.fit(X_train, y_train)

        y_pred = model.predict(X_test)

        score = accuracy_score(y_test, y_pred)

        scores.append(score)

    avg_score = sum(scores) / len(scores)

    # Print the average score
    st.write(f"å¹³å‡å‡†ç¡®ç‡: {avg_score:.2f}")


if __name__ == '__main__':

    df = Uploaded()

    col1, col2, col3 = st.columns(3)

    with col1:
        st.header("æœ´ç´ è´å¶æ–¯ç®—æ³•")
        if df is not None:
            X_train, x_test, Y_train, y_test = process(df, mode=0)

            state = st.write("æ­£åœ¨è¿›è¡Œè®­ç»ƒ...")
            bys=Bayes_classfier()
            bys.fit(X_train, Y_train)
            state = st.write("è®­ç»ƒç»“æŸ")
            pred = bys.predict(x_test)
            accuracy_bys = accuracy_score(pred, y_test)

            st.success("æµ‹è¯•é›†å‡†ç¡®ç‡: %.3f%%" % (accuracy_bys * 100))

            K_fold_validation(X_train,Y_train,bys)
#=============================================ç»˜å›¾
            precision_bp = precision_score(y_test, pred, average='macro')
            recall_bp = recall_score(y_test, pred, average='macro')
            f1_score_bp = f1_score(y_test, pred, average='macro')

            fig_bp, axis_bp = plt.subplots()
            fpr, tpr, thresholds = roc_curve(y_test, pred)

            # è®¡ç®—AUCå€¼
            roc_auc = auc(fpr, tpr)
            # ç»˜åˆ¶ROCæ›²çº¿
            axis_bp.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
            axis_bp.set_ylabel('ROC')
            axis_bp.legend()
            st.pyplot(fig_bp)

    with col2:
        st.header("ID3å†³ç­–æ ‘")
        if df is not None:
            X_train, x_test, Y_train, y_test = process(df, mode=0)
            state = st.write("æ­£åœ¨è¿›è¡Œè®­ç»ƒ...")

            dt = DecisionTree(max_depth=10)
            dt.fit(X_train, Y_train)
            pred = dt.predict(x_test)  # Output: [0 1 1 0]
            accuracy_id3= accuracy_score(y_test, pred)
            st.write("è®­ç»ƒç»“æŸ")
            st.success("æµ‹è¯•é›†å‡†ç¡®ç‡:{}".format(accuracy_id3 * 100))

            K_fold_validation(X_train,Y_train,dt)

            precision_svm = precision_score(y_test, pred, average='macro')
            recall_svm = recall_score(y_test, pred, average='macro')
            f1_score_svm = f1_score(y_test, pred, average='macro')

            fig_svm, axis_svm = plt.subplots()
            fpr, tpr, thresholds = roc_curve(y_test, pred)

            # è®¡ç®—AUCå€¼
            roc_auc = auc(fpr, tpr)
            # ç»˜åˆ¶ROCæ›²çº¿
            axis_svm.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
            axis_svm.set_ylabel('ROC')
            axis_svm.legend()
            st.pyplot(fig_svm)
    with col3:
        st.header("knnç®—æ³•")
        if df is not None:
            X_train, x_test, Y_train, y_test = process(df, mode=0)

            np.random.seed(123)
            knn=KNN(k=3)
            state = st.write("æ­£åœ¨è¿›è¡Œè®­ç»ƒ...")
            knn.fit(X_train, Y_train)
            state = st.write("è®­ç»ƒç»“æŸ")
            # é¢„æµ‹æ–°æ•°
            y_pred = knn.predict(x_test)
            accuracy_knn = accuracy_score(y_test, y_pred)
            st.success("æµ‹è¯•é›†å‡†ç¡®ç‡: %.3f%%" % (accuracy_knn * 100))

            K_fold_validation(X_train, Y_train, knn)

            precision_lr = precision_score(y_test, y_pred, average='macro')
            recall_lr = recall_score(y_test, y_pred, average='macro')
            f1_score_lr = f1_score(y_test, y_pred, average='macro')

            fig_lr, axis_lr = plt.subplots()
            fpr, tpr, thresholds = roc_curve(y_test, y_pred)

            # è®¡ç®—AUCå€¼
            roc_auc = auc(fpr, tpr)
            # ç»˜åˆ¶ROCæ›²çº¿
            axis_lr.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
            axis_lr.set_ylabel('ROC')
            axis_lr.legend()
            st.pyplot(fig_lr)


if st.button("æ¨¡å‹è¯„ä»·"):

    labels = ['Accuracy', 'Precision', 'Recall', 'F1']
    bp_scores = [accuracy_bys, precision_bp, recall_bp, f1_score_bp]
    svm_scores = [accuracy_id3, precision_svm, recall_svm, f1_score_svm]
    lr_scores = [accuracy_knn, precision_lr, recall_lr, f1_score_lr]

    x = np.arange(len(labels))
    width = 0.25


    st.title("ç®—æ³•è¯„ä»·æ¯”è¾ƒ")
    st.write("ä¸‰ä¸ªåˆ†ç±»ç®—æ³•åœ¨æµ‹è¯•é›†ä¸Šçš„æŒ‡æ ‡æ¯”è¾ƒï¼š")

    # åˆ›å»ºä¸€ä¸ªMatplotlibå›¾è¡¨
    fig, ax = plt.subplots()
    # åœ¨å›¾è¡¨ä¸Šç»˜åˆ¶æŸ±çŠ¶å›¾
    rects1 = ax.bar(x - width, bp_scores, width, label='BP')
    rects2 = ax.bar(x, svm_scores, width, label='SVM')
    rects3 = ax.bar(x + width, lr_scores, width, label='LR')

    # è®¾ç½®å›¾è¡¨çš„æ ‡é¢˜å’Œè½´æ ‡ç­¾
    ax.set_ylabel('Scores')
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend()

    # å°†å›¾è¡¨è½¬æ¢ä¸ºå›¾åƒï¼Œå¹¶åœ¨Streamlité¡µé¢ä¸Šæ˜¾ç¤º
    st.pyplot(fig)
